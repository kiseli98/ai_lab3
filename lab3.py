# -*- coding: utf-8 -*-
"""lab3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YaQHq-xpHSUCC4SNkeXKMhwkzw8YaNSz
"""

!pip install -q seaborn

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score


file_url = 'https://raw.githubusercontent.com/kiseli98/ai_lab3/main/apartmentComplexData%20-%20Copy.csv'
column_names = ['complexAge', 'totalRooms', 'totalBedrooms', 'complexInhabitants', 'apartmentsNr', 'medianCompexValue']
raw_dataset = pd.read_csv(file_url, names=column_names)

dataset = raw_dataset.copy()
dataset.tail()

train_dataset = dataset.sample(frac=0.9, random_state=0)
test_dataset = dataset.drop(train_dataset.index)

# train_dataset = dataset.sample(n = 5000)
# test_dataset = dataset.drop(train_dataset.index)
# test_dataset = test_dataset.sample(n = 200)

print(len(train_dataset.index))
print(len(test_dataset.index))

"""## Inspecting the data

Looking at the joint distribution of a few pairs of columns from the training set in order to determine dependencies between features and **labels**
"""

sns.pairplot(train_dataset[column_names], diag_kind='kde')

train_dataset.describe().transpose()

train_features = train_dataset.copy()
test_features = test_dataset.copy()

# Separate the target value (label) from the features. 
# Model will predict label
train_labels = train_features.pop('medianCompexValue')
test_labels = test_features.pop('medianCompexValue')

train_dataset.describe().transpose()[['mean', 'std']]

"""The ranges of each feature differ a lot between themselves, so normalization can be applied to the data. It is good practice to normalize features that use different scales and ranges."""

normalizer = preprocessing.Normalization()
normalizer.adapt(np.array(train_features))


first_sample = np.array(train_features[:10])
# for sample in first_sample:
with np.printoptions(precision=6, suppress=True):
  print('First example:', first_sample)
  print()
  print('Normalized:', normalizer(first_sample).numpy())

"""Multi-input linear regression is the application of a linear transformation $(y=mx+b)$ to produce 1 output, where $m$ is matrix and $b$ is vector.

Keras Sequential model represents a sequence of steps:


*   Data normalization
*   Application of linear transformation 


"""

linear_model = tf.keras.Sequential([
    normalizer,
    layers.Dense(units=1)
])

linear_model.summary()

"""Running the untrained model on the first 10 values"""

linear_model.predict(train_features[:10])

linear_model.layers[1].kernel

"""### Training model configuration 
Mean absolute error will be optimized using Adam algorithm. Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.
"""

linear_model.compile(
    optimizer=tf.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

history = linear_model.fit(
    train_features, train_labels, 
    epochs=200,
    verbose=0,
    # Calculate validation results on 20% of the training data
    validation_split = 0.2)

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  plt.ylim([0, 900000])
  plt.xlabel('Epoch')
  plt.ylabel('Error [medianCompexValue]')
  plt.legend()
  plt.grid(True)

plot_loss(history)

test_results = {}
test_results['linear_model'] = linear_model.evaluate(
    test_features, test_labels, verbose=0)

pd.DataFrame(test_results, index=['Mean absolute error [medianCompexValue]']).T

test_predictions = linear_model.predict(test_features).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [medianCompexValue]')
plt.ylabel('Predictions [medianCompexValue]')
lims = [0, 100000]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)

# column_names = ['complexAge', 'totalRooms', 'totalBedrooms', 'complexInhabitants', 'apartmentsNr']
user_input = [[21,916,194,451,178]]   # should be 63300 - original value
prediction =  linear_model.predict(user_input).flatten()
print('Predicted value: ',prediction)

"""# DNN Model"""

def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model

dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()

history = dnn_model.fit(
    train_features, train_labels,
    validation_split=0.2,
    verbose=0, epochs=100)

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

plot_loss(history)

test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)

pd.DataFrame(test_results, index=['Mean absolute error [medianCompexValue]']).T

test_predictions_dnn = dnn_model.predict(test_features).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions_dnn)
plt.xlabel('True Values [medianCompexValue]')
plt.ylabel('Predictions [medianCompexValue]')
lims = [0, 500000]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)

# column_names = ['complexAge', 'totalRooms', 'totalBedrooms', 'complexInhabitants', 'apartmentsNr']
user_input = [[21,916,194,451,178]]   # should be 63300 - original value
prediction =  dnn_model.predict(user_input).flatten()
print('Predicted value: ',prediction)

"""## Linear Regression using scikit learn"""

# Commented out IPython magic to ensure Python compatibility.
column_names = ['complexAge', 'totalRooms', 'totalBedrooms', 'complexInhabitants', 'apartmentsNr', 'medianCompexValue']
feature_cols = ['complexAge', 'totalRooms', 'totalBedrooms', 'complexInhabitants', 'apartmentsNr']
data = pd.read_csv('https://raw.githubusercontent.com/kiseli98/ai_lab3/main/apartmentComplexData%20-%20Copy.csv', names=column_names)


train_dataset = data.sample(frac=0.9, random_state=0)
test_dataset = data.drop(train_dataset.index)

# train_dataset = data.sample(n = 100)
# test_dataset = data.drop(train_dataset.index)
# test_dataset = test_dataset.sample(n = 20)

train_features = train_dataset.copy()
test_features = test_dataset.copy()

# Separate the target value (label) from the features. 
# Model will predict label
train_labels = train_features.pop('medianCompexValue')
test_labels = test_features.pop('medianCompexValue')

lr_model = linear_model.LinearRegression()

# Train the model using the training sets
lr_model.fit(train_features, train_labels)

# Make predictions using the testing set
prediction = lr_model.predict(test_features)

print('Coefficients: \n', lr_model.coef_)
print('Mean squared error: %.2f'
#       % mean_squared_error(test_labels, prediction))
print('Coefficient of determination: %.2f'
#       % r2_score(test_labels, prediction))

a = plt.axes(aspect='equal')
plt.scatter(test_labels, prediction)
plt.xlabel('True Values [medianCompexValue]')
plt.ylabel('Predictions [medianCompexValue]')
lims = [0, 500000]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)

#predict for custom variables
user_input = [[21,916,194,451,178]]   # should be 63300 - original value
prediction =  lr_model.predict(user_input).flatten()
print('Predicted value: ',prediction)